{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d9338df-0364-40ef-ba69-d91db2602f4c",
   "metadata": {},
   "source": [
    "#### Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a16398c-05eb-46cf-ab14-788c8fee1ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tempfile\n",
    "from tqdm import tqdm\n",
    "import gcsfs\n",
    "import mediapipe as mp\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "BUCKET_NAME = \"computer-vision-security-sys-data-hj\" # Your bucket name\n",
    "GCS_ACTIONS_PATH = f\"gs://{BUCKET_NAME}/raw/actions/\"\n",
    "# The number of frames we'll use for each video sequence\n",
    "SEQUENCE_LENGTH = 45 \n",
    "# --- END CONFIGURATION ---\n",
    "\n",
    "# Initialize GCS filesystem and MediaPipe\n",
    "fs = gcsfs.GCSFileSystem()\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "print(\"Setup Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059b3820-935a-46be-a4ad-f32b5c95c03f",
   "metadata": {},
   "source": [
    "#### Keypoint Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0883df7-a3b9-4bf4-9f4c-b16ae4e7659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints_from_video(gcs_video_path):\n",
    "    \"\"\"\n",
    "    Downloads a video from GCS, extracts pose keypoints for each frame,\n",
    "    and returns them as a sequence. This version includes a fix for Windows file locking.\n",
    "    \"\"\"\n",
    "    sequence = []\n",
    "    \n",
    "    # 1. Create a temporary file but tell it not to delete automatically\n",
    "    tmp_file = tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False)\n",
    "    local_video_path = tmp_file.name\n",
    "    \n",
    "    # 2. Immediately close the file to release the Windows lock\n",
    "    tmp_file.close()\n",
    "\n",
    "    try:\n",
    "        # 3. Download the GCS file to the now-unlocked local path\n",
    "        fs.get(gcs_video_path, local_video_path)\n",
    "        \n",
    "        # 4. OpenCV can now safely open and read the local video file\n",
    "        cap = cv2.VideoCapture(local_video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Error opening video file: {gcs_video_path}\")\n",
    "            return None\n",
    "            \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            # Convert the frame to RGB for MediaPipe\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False # Performance optimization\n",
    "            \n",
    "            # Make detection\n",
    "            results = pose.process(image)\n",
    "            \n",
    "            # Extract landmarks\n",
    "            if results.pose_landmarks:\n",
    "                nose = results.pose_landmarks.landmark[mp_pose.PoseLandmark.NOSE]\n",
    "                sequence.append([nose.x, nose.y])\n",
    "            else:\n",
    "                sequence.append([0.0, 0.0])\n",
    "\n",
    "        cap.release()\n",
    "        \n",
    "    finally:\n",
    "        # 5. ALWAYS clean up the temporary file when we're done\n",
    "        os.remove(local_video_path)\n",
    "        \n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444bce14-c9e6-4aef-8cb2-cdf70257f4e2",
   "metadata": {},
   "source": [
    "#### Process All Your Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1066ffd6-99c3-41a3-b6e4-eb883fc6a27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found action folders: ['entering', 'leaving', 'other']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 'entering' videos: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [01:48<00:00, 18.11s/it]\n",
      "Processing 'leaving' videos: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [01:53<00:00, 18.89s/it]\n",
      "Processing 'other' videos: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [02:28<00:00, 24.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete. Total sequences extracted: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Lists to hold all our data\n",
    "sequences = []\n",
    "labels = []\n",
    "\n",
    "action_folders = [os.path.basename(f) for f in fs.ls(GCS_ACTIONS_PATH) if fs.isdir(f)]\n",
    "print(f\"Found action folders: {action_folders}\")\n",
    "\n",
    "for action in action_folders:\n",
    "    action_path = os.path.join(GCS_ACTIONS_PATH, action)\n",
    "    video_files = [f\"gs://{f}\" for f in fs.ls(action_path) if f.endswith('.mp4')]\n",
    "    \n",
    "    for video_path in tqdm(video_files, desc=f\"Processing '{action}' videos\"):\n",
    "        keypoints = extract_keypoints_from_video(video_path)\n",
    "        if keypoints:\n",
    "            sequences.append(keypoints)\n",
    "            labels.append(action)\n",
    "\n",
    "print(f\"\\nProcessing complete. Total sequences extracted: {len(sequences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0cb2c9-7003-4f27-8f15-aa6cc2d02a3d",
   "metadata": {},
   "source": [
    "#### Data Preparation (Padding, Encoding, Splitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af490de1-a75f-45d9-a566-61398998a7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shapes:\n",
      "X_train: (14, 45, 2)\n",
      "X_test: (4, 45, 2)\n",
      "y_train: (14, 3)\n",
      "y_test: (4, 3)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Pad or truncate sequences to our fixed SEQUENCE_LENGTH\n",
    "X = pad_sequences(sequences, maxlen=SEQUENCE_LENGTH, padding='post', truncating='post', dtype='float32')\n",
    "\n",
    "# Convert text labels to one-hot encoded vectors\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(labels)\n",
    "y = to_categorical(y_encoded)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Data shapes:\")\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d819924b-6b25-4592-b28e-46b507f77845",
   "metadata": {},
   "source": [
    "#### Model Architecture and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed224544-bf96-41e1-ad90-4ebf8f374b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\himan\\Documents\\Projects\\computer_vision_security_system\\dev\\computer-vision-security-sys\\venv\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"lstm_2\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LSTM, Dense\n\u001b[0;32m      4\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(label_encoder\u001b[38;5;241m.\u001b[39mclasses_)\n\u001b[1;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSequential\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mLSTM\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSEQUENCE_LENGTH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mLSTM\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mLSTM\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msoftmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "File \u001b[1;32m~\\Documents\\Projects\\computer_vision_security_system\\dev\\computer-vision-security-sys\\venv\\lib\\site-packages\\keras\\src\\models\\sequential.py:76\u001b[0m, in \u001b[0;36mSequential.__init__\u001b[1;34m(self, layers, trainable, name)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m layers:\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd(layer, rebuild\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_rebuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\Projects\\computer_vision_security_system\\dev\\computer-vision-security-sys\\venv\\lib\\site-packages\\keras\\src\\models\\sequential.py:149\u001b[0m, in \u001b[0;36mSequential._maybe_rebuild\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m0\u001b[39m], InputLayer) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    148\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbatch_shape\n\u001b[1;32m--> 149\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;66;03m# We can build the Sequential model if the first layer has the\u001b[39;00m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;66;03m# `input_shape` property. This is most commonly found in Functional\u001b[39;00m\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;66;03m# model.\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39minput_shape\n",
      "File \u001b[1;32m~\\Documents\\Projects\\computer_vision_security_system\\dev\\computer-vision-security-sys\\venv\\lib\\site-packages\\keras\\src\\layers\\layer.py:231\u001b[0m, in \u001b[0;36mLayer.__new__.<locals>.build_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_open_name_scope():\n\u001b[0;32m    230\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m current_path()\n\u001b[1;32m--> 231\u001b[0m     original_build_method(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    232\u001b[0m \u001b[38;5;66;03m# Record build config.\u001b[39;00m\n\u001b[0;32m    233\u001b[0m signature \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(original_build_method)\n",
      "File \u001b[1;32m~\\Documents\\Projects\\computer_vision_security_system\\dev\\computer-vision-security-sys\\venv\\lib\\site-packages\\keras\\src\\models\\sequential.py:195\u001b[0m, in \u001b[0;36mSequential.build\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 195\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;66;03m# Can happen if shape inference is not implemented.\u001b[39;00m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO: consider reverting inbound nodes on layers processed.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\Projects\\computer_vision_security_system\\dev\\computer-vision-security-sys\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\Documents\\Projects\\computer_vision_security_system\\dev\\computer-vision-security-sys\\venv\\lib\\site-packages\\keras\\src\\layers\\input_spec.py:186\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mallow_last_axis_squeeze:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;241m!=\u001b[39m spec\u001b[38;5;241m.\u001b[39mndim:\n\u001b[1;32m--> 186\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    187\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    188\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis incompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    189\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, found ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    190\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull shape received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    191\u001b[0m         )\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mmax_ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m ndim \u001b[38;5;241m>\u001b[39m spec\u001b[38;5;241m.\u001b[39mmax_ndim:\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"lstm_2\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 128)"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(128, return_sequences=True, activation='relu', input_shape=(SEQUENCE_LENGTH, 2)),\n",
    "    LSTM(128, activation='relu'),\n",
    "    LSTM(128, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "print(\"\\n--- Starting Model Training ---\")\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), batch_size=16)\n",
    "print(\"--- Model Training Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30efe2ce-78a2-4701-a8ac-0ad877c6cf38",
   "metadata": {},
   "source": [
    "#### Evaluation and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d3833b-9b4b-420a-9fe9-98afaf39f154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"\\nModel Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"../src/processor/action_model.h5\")\n",
    "print(\"✅ Model saved to ../src/processor/action_model.h5\")\n",
    "\n",
    "# IMPORTANT: Save the label encoder as well!\n",
    "# We need it to decode the model's predictions back to text.\n",
    "with open('../src/processor/action_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "print(\"✅ Label encoder saved to ../src/processor/action_encoder.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54461a24-523e-40e0-a4de-12ec88c91374",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Computer Vision Venv",
   "language": "python",
   "name": "computer-vision-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
